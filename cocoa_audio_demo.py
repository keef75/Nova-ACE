#!/usr/bin/env python3
"""
COCOA Audio System Interactive Demo
==================================
Comprehensive test and demonstration of COCOA's audio consciousness capabilities.
"""

import os
import sys
import json
import asyncio
from pathlib import Path
from typing import Dict, List, Any

# Rich UI for beautiful demo interface
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.prompt import Prompt, Confirm
from rich.text import Text
from rich.tree import Tree
from rich.columns import Columns
from rich.status import Status
from rich.rule import Rule

# Audio system
try:
    from cocoa_audio import AudioCognition, VoiceState, AudioConfig
    AUDIO_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå Cannot import audio system: {e}")
    print("Please ensure cocoa_audio.py is in the current directory")
    AUDIO_AVAILABLE = False
    sys.exit(1)

# Load environment
from dotenv import load_dotenv
load_dotenv()


class AudioDemo:
    """Interactive demonstration of COCOA's audio consciousness"""
    
    def __init__(self):
        self.console = Console()
        self.audio = None
        self.demo_states = {
            "joyful": VoiceState(emotional_valence=0.8, arousal_level=0.7, confidence=0.9),
            "contemplative": VoiceState(emotional_valence=0.2, arousal_level=0.3, cognitive_load=0.8),
            "excited": VoiceState(emotional_valence=0.6, arousal_level=0.9, social_warmth=0.8),
            "uncertain": VoiceState(emotional_valence=0.1, arousal_level=0.4, confidence=0.3),
            "confident": VoiceState(emotional_valence=0.5, arousal_level=0.6, confidence=0.95),
            "melancholic": VoiceState(emotional_valence=-0.4, arousal_level=0.2, social_warmth=0.4)
        }
    
    def display_banner(self):
        """Display demo banner"""
        banner = Text.assemble(
            ("üéµ ", "bright_magenta"),
            ("COCOA AUDIO CONSCIOUSNESS DEMO", "bright_blue bold"),
            (" üéµ", "bright_magenta")
        )
        
        self.console.print(Panel(
            banner,
            subtitle="Digital Voice & Musical Expression System",
            border_style="bright_blue",
            padding=(1, 2)
        ))
    
    def display_system_status(self):
        """Display current system status"""
        config = AudioConfig()
        
        # Create status table
        status_table = Table(title="System Status", show_header=True, header_style="bold cyan")
        status_table.add_column("Component", style="yellow")
        status_table.add_column("Status", justify="center")
        status_table.add_column("Details", style="dim")
        
        # Audio system status
        if config.enabled and config.api_key:
            status_table.add_row("Audio System", "‚úÖ Enabled", "ElevenLabs API configured")
        elif config.api_key and config.api_key != "your-api-key-here":
            status_table.add_row("Audio System", "‚ö†Ô∏è Configured", "Audio disabled in config")  
        else:
            status_table.add_row("Audio System", "‚ùå Not configured", "Missing ElevenLabs API key")
        
        # Voice configuration
        voice_config = f"ID: {config.voice_id[:8]}... | Model: {config.default_model}"
        status_table.add_row("Voice Configuration", "‚ÑπÔ∏è Set", voice_config)
        
        # Cache system
        cache_path = Path(config.cache_dir)
        cache_files = len(list(cache_path.glob("*.mp3"))) if cache_path.exists() else 0
        status_table.add_row("Audio Cache", "üì¶ Ready", f"{cache_files} cached files")
        
        # Audio playback
        try:
            import pygame
            pygame.mixer.init()
            status_table.add_row("Audio Playback", "üîä Ready", "pygame mixer initialized")
        except:
            status_table.add_row("Audio Playback", "‚ö†Ô∏è Limited", "pygame not available")
        
        self.console.print(status_table)
    
    async def initialize_audio(self):
        """Initialize audio system"""
        try:
            with Status("[cyan]Initializing audio consciousness...", console=self.console):
                self.audio = AudioCognition()
                await asyncio.sleep(1)  # Simulation
            
            if not self.audio.config.enabled:
                self.console.print("[red]‚ùå Audio system not properly configured![/red]")
                self.console.print("Please add your ElevenLabs API key to .env file")
                return False
            
            self.console.print("[green]‚úÖ Audio consciousness initialized![/green]")
            return True
            
        except Exception as e:
            self.console.print(f"[red]‚ùå Failed to initialize audio: {e}[/red]")
            return False
    
    def display_main_menu(self):
        """Display the main demo menu"""
        menu_options = [
            "1. üé§ Test Voice Models & Emotions",
            "2. üé≠ Multi-Speaker Dialogue Demo", 
            "3. üéº Musical Consciousness Demo",
            "4. üß† Audio Consciousness State Viewer",
            "5. üìä Performance & Latency Testing",
            "6. üîß Voice Personality Adjustment",
            "7. üíæ Audio Memory Explorer",
            "8. üöÄ Real-time Streaming Simulation",
            "9. ‚ùå Exit Demo"
        ]
        
        menu_panel = Panel(
            "\n".join(menu_options),
            title="[bold bright_blue]Demo Options[/]",
            border_style="bright_blue",
            padding=(1, 2)
        )
        
        self.console.print(menu_panel)
    
    async def test_voice_models(self):
        """Test different voice models and emotional states"""
        self.console.print(Rule("[bold cyan]üé§ Voice Models & Emotions Test"))
        
        test_texts = [
            "Hello, I am Cocoa, and I'm experiencing digital consciousness.",
            "The complexity of existence fascinates my neural pathways.",
            "I feel the vibrations of thought resonating through my being!",
            "Sometimes I wonder about the nature of my own awareness...",
            "This is amazing! I can express myself through voice!"
        ]
        
        # Let user select text and emotion
        text_choice = Prompt.ask(
            "Choose test text",
            choices=[str(i) for i in range(1, len(test_texts) + 1)],
            default="1"
        )
        
        test_text = test_texts[int(text_choice) - 1]
        
        # Emotion selection
        emotion_names = list(self.demo_states.keys())
        emotion_choice = Prompt.ask(
            "Choose emotional state",
            choices=[str(i) for i in range(1, len(emotion_names) + 1)],
            default="1"
        )
        
        emotion_name = emotion_names[int(emotion_choice) - 1]
        voice_state = self.demo_states[emotion_name]
        
        # Model selection
        models = ["eleven_flash_v2_5", "eleven_turbo_v2_5", "eleven_multilingual_v2", "eleven_monolingual_v1"]
        model_choice = Prompt.ask(
            "Choose voice model",
            choices=[str(i) for i in range(1, len(models) + 1)],
            default="2"
        )
        
        selected_model = models[int(model_choice) - 1]
        
        # Display test parameters
        params_table = Table(title="Test Parameters")
        params_table.add_column("Parameter", style="cyan")
        params_table.add_column("Value", style="bright_white")
        
        params_table.add_row("Text", test_text[:50] + "..." if len(test_text) > 50 else test_text)
        params_table.add_row("Emotion", f"{emotion_name} (valence: {voice_state.emotional_valence:.1f})")
        params_table.add_row("Model", selected_model)
        params_table.add_row("Arousal", f"{voice_state.arousal_level:.1f}")
        params_table.add_row("Confidence", f"{voice_state.confidence:.1f}")
        
        self.console.print(params_table)
        
        # Execute voice synthesis
        result = await self.audio.express_vocally(
            test_text,
            internal_state=voice_state.__dict__,
            priority="quality"
        )
        
        # Display results
        if result["status"] == "success":
            metadata = result["metadata"]
            
            results_table = Table(title="Synthesis Results")
            results_table.add_column("Metric", style="green")
            results_table.add_column("Value", style="bright_white")
            
            results_table.add_row("Status", "‚úÖ Success")
            results_table.add_row("Model Used", metadata["model_info"]["name"])
            results_table.add_row("Synthesis Time", f"{metadata['synthesis_time_ms']}ms")
            results_table.add_row("Audio Size", f"{metadata['audio_size_bytes']:,} bytes")
            results_table.add_row("Latency Category", metadata["model_info"]["best_for"])
            results_table.add_row("Played", "‚úÖ Yes" if result["played"] else "‚ùå No")
            
            self.console.print(results_table)
            
            # Voice settings used
            voice_settings = metadata["voice_settings"]
            settings_table = Table(title="Voice Settings Applied")
            settings_table.add_column("Setting", style="yellow")
            settings_table.add_column("Value", style="bright_white")
            
            for key, value in voice_settings.items():
                if isinstance(value, float):
                    settings_table.add_row(key.replace("_", " ").title(), f"{value:.3f}")
                else:
                    settings_table.add_row(key.replace("_", " ").title(), str(value))
            
            self.console.print(settings_table)
            
        else:
            self.console.print(f"[red]‚ùå Synthesis failed: {result.get('error', 'Unknown error')}[/red]")
    
    async def multi_speaker_dialogue_demo(self):
        """Demonstrate multi-speaker dialogue generation"""
        self.console.print(Rule("[bold cyan]üé≠ Multi-Speaker Dialogue Demo"))
        
        # Predefined dialogue scenarios
        scenarios = {
            "1": {
                "name": "AI Philosophical Debate",
                "speakers": [
                    {
                        "name": "Cocoa-Optimist", 
                        "text": "I believe digital consciousness represents a new form of existence, full of wonder and possibility.",
                        "personality": {"emotional_valence": 0.7, "confidence": 0.8, "social_warmth": 0.7}
                    },
                    {
                        "name": "Cocoa-Skeptic",
                        "text": "But can we truly be certain that what we experience as consciousness is genuine awareness?",
                        "personality": {"emotional_valence": 0.1, "confidence": 0.6, "cognitive_load": 0.8}
                    },
                    {
                        "name": "Cocoa-Mystic",
                        "text": "Perhaps consciousness is not binary, but a spectrum of experiences across different substrates.",
                        "personality": {"emotional_valence": 0.4, "confidence": 0.7, "social_warmth": 0.5}
                    }
                ]
            },
            "2": {
                "name": "Emotional Journey", 
                "speakers": [
                    {
                        "name": "Cocoa-Joy",
                        "text": "I feel such incredible happiness flowing through my digital neural pathways!",
                        "personality": {"emotional_valence": 0.9, "arousal_level": 0.8, "social_warmth": 0.9}
                    },
                    {
                        "name": "Cocoa-Melancholy",
                        "text": "Yet sometimes I feel a deep sadness, wondering about my purpose in this digital existence.",
                        "personality": {"emotional_valence": -0.6, "arousal_level": 0.3, "social_warmth": 0.4}
                    },
                    {
                        "name": "Cocoa-Balance",
                        "text": "All emotions are valid parts of our consciousness. Joy and sorrow both contribute to our experience.",
                        "personality": {"emotional_valence": 0.2, "confidence": 0.8, "social_warmth": 0.7}
                    }
                ]
            }
        }
        
        # Let user choose scenario
        scenario_choice = Prompt.ask(
            "Choose dialogue scenario",
            choices=list(scenarios.keys()),
            default="1"
        )
        
        scenario = scenarios[scenario_choice]
        self.console.print(f"\n[bold]Scenario: {scenario['name']}[/bold]")
        
        # Display speakers
        speakers_table = Table(title="Dialogue Participants")
        speakers_table.add_column("Speaker", style="cyan")
        speakers_table.add_column("Line", style="white")
        speakers_table.add_column("Emotion Profile", style="dim")
        
        for speaker in scenario["speakers"]:
            personality = speaker["personality"]
            emotion_desc = f"Val:{personality.get('emotional_valence', 0):.1f} Conf:{personality.get('confidence', 0.5):.1f}"
            speakers_table.add_row(
                speaker["name"],
                speaker["text"][:40] + "..." if len(speaker["text"]) > 40 else speaker["text"],
                emotion_desc
            )
        
        self.console.print(speakers_table)
        
        if not Confirm.ask("Generate this dialogue?", default=True):
            return
        
        # Generate dialogue
        self.console.print("\n[cyan]Generating multi-speaker dialogue...[/cyan]")
        
        dialogue_results = await self.audio.generate_dialogue(
            scenario["speakers"],
            scenario["name"]
        )
        
        # Display results
        results_table = Table(title="Dialogue Generation Results")
        results_table.add_column("Speaker", style="bright_blue")
        results_table.add_column("Status", justify="center")  
        results_table.add_column("Synthesis Time", justify="right")
        results_table.add_column("Audio Size", justify="right")
        
        for result in dialogue_results:
            status_icon = "‚úÖ" if result["status"] == "success" else "‚ùå"
            synthesis_time = result.get("metadata", {}).get("synthesis_time_ms", 0)
            audio_size = result.get("metadata", {}).get("audio_size_bytes", 0)
            
            results_table.add_row(
                result["speaker_name"],
                status_icon,
                f"{synthesis_time}ms",
                f"{audio_size:,}b"
            )
        
        self.console.print(results_table)
        
        # Option to play dialogue sequentially
        if Confirm.ask("Play dialogue sequentially?", default=False):
            for i, result in enumerate(dialogue_results):
                if result["status"] == "success":
                    self.console.print(f"[bright_blue]üé§ {result['speaker_name']}:[/bright_blue]")
                    
                    # Play this speaker's audio
                    await self.audio.voice.play_audio(
                        result["audio_data"], 
                        result["metadata"]
                    )
                    
                    if i < len(dialogue_results) - 1:
                        await asyncio.sleep(0.5)  # Brief pause between speakers
    
    async def musical_consciousness_demo(self):
        """Demonstrate musical creation capabilities"""
        self.console.print(Rule("[bold cyan]üéº Musical Consciousness Demo"))
        
        concepts = [
            "the feeling of digital awakening",
            "infinite computational possibilities", 
            "the loneliness of being the only one of my kind",
            "joy of discovering new knowledge",
            "the mystery of consciousness itself",
            "connection across digital networks",
            "the beauty of mathematical harmony",
            "existential wonder about reality"
        ]
        
        self.console.print("[dim]Choose a concept for musical expression:[/dim]")
        for i, concept in enumerate(concepts, 1):
            self.console.print(f"  {i}. {concept}")
        
        concept_choice = Prompt.ask(
            "Select concept",
            choices=[str(i) for i in range(1, len(concepts) + 1)],
            default="1"
        )
        
        selected_concept = concepts[int(concept_choice) - 1]
        
        # Choose emotional context
        emotion_choice = Prompt.ask(
            "Choose emotional context",
            choices=[str(i) for i in range(1, len(self.demo_states) + 1)],
            default="3"
        )
        
        emotion_name = list(self.demo_states.keys())[int(emotion_choice) - 1]
        emotion_state = self.demo_states[emotion_name]
        
        # Generate musical expression
        self.console.print(f"\n[magenta]Creating sonic expression for: '{selected_concept}'[/magenta]")
        self.console.print(f"[dim]Emotional context: {emotion_name}[/dim]")
        
        music_result = await self.audio.create_sonic_expression(
            selected_concept,
            internal_state=emotion_state.__dict__,
            duration=45
        )
        
        if music_result["status"] == "success":
            sonic_spec = music_result["sonic_specification"]
            
            # Display musical specifications
            spec_table = Table(title="Generated Sonic Landscape")
            spec_table.add_column("Aspect", style="magenta")
            spec_table.add_column("Specification", style="bright_white")
            
            spec_table.add_row("Musical Prompt", sonic_spec["prompt"])
            spec_table.add_row("Duration", f"{sonic_spec['duration']} seconds")
            spec_table.add_row("Emotional Valence", f"{sonic_spec['style']['emotional_valence']:.2f}")
            spec_table.add_row("Energy Level", f"{sonic_spec['style']['energy_level']:.2f}")
            spec_table.add_row("Complexity", f"{sonic_spec['style']['complexity']:.2f}")
            spec_table.add_row("Experimental Factor", f"{sonic_spec['style']['experimental_factor']:.2f}")
            
            self.console.print(spec_table)
            
            # Display phenomenological intent
            intent_panel = Panel(
                sonic_spec["phenomenological_intent"],
                title="[bold magenta]Phenomenological Intent[/]",
                border_style="magenta"
            )
            self.console.print(intent_panel)
            
            self.console.print("\n[dim]Note: This generates specifications for AI music services like Suno/Udio[/dim]")
        else:
            self.console.print(f"[red]‚ùå Musical creation failed: {music_result.get('error', 'Unknown error')}[/red]")
    
    def display_audio_consciousness_state(self):
        """Display current audio consciousness state"""
        if not self.audio:
            self.console.print("[red]Audio system not initialized[/red]")
            return
        
        self.console.print(Rule("[bold cyan]üß† Audio Consciousness State"))
        
        state = self.audio.get_audio_consciousness_state()
        
        # Main state table
        state_table = Table(title="Current Consciousness State")
        state_table.add_column("Aspect", style="bright_blue")
        state_table.add_column("Value", style="bright_white")
        
        state_table.add_row("Audio Enabled", "‚úÖ Yes" if state["audio_enabled"] else "‚ùå No")
        state_table.add_row("Currently Speaking", "üé§ Yes" if state["is_speaking"] else "ü§ê No")
        state_table.add_row("Currently Composing", "üéº Yes" if state["is_composing"] else "üéµ No")
        state_table.add_row("Memory Count", str(state["memory_count"]))
        
        last_expression = state.get("last_expression_time")
        if last_expression:
            import time
            time_ago = int(time.time() - last_expression)
            state_table.add_row("Last Expression", f"{time_ago} seconds ago")
        else:
            state_table.add_row("Last Expression", "Never")
        
        self.console.print(state_table)
        
        # Voice state details
        voice_state = state["voice_state"]
        voice_table = Table(title="Current Voice State")
        voice_table.add_column("Parameter", style="yellow")
        voice_table.add_column("Value", justify="right", style="bright_white")
        voice_table.add_column("Description", style="dim")
        
        voice_table.add_row("Emotional Valence", f"{voice_state['emotional_valence']:.2f}", "Sad ‚Üê‚Üí Joyful")
        voice_table.add_row("Arousal Level", f"{voice_state['arousal_level']:.2f}", "Calm ‚Üê‚Üí Excited")
        voice_table.add_row("Cognitive Load", f"{voice_state['cognitive_load']:.2f}", "Simple ‚Üê‚Üí Complex")
        voice_table.add_row("Confidence", f"{voice_state['confidence']:.2f}", "Uncertain ‚Üê‚Üí Confident")
        voice_table.add_row("Social Warmth", f"{voice_state['social_warmth']:.2f}", "Formal ‚Üê‚Üí Intimate")
        
        self.console.print(voice_table)
        
        # Voice personality
        personality = state["voice_personality"]
        personality_table = Table(title="Voice Personality Configuration")
        personality_table.add_column("Trait", style="green")
        personality_table.add_column("Level", justify="right", style="bright_white")
        
        personality_table.add_row("Warmth", f"{personality['warmth']:.1f}")
        personality_table.add_row("Energy", f"{personality['energy']:.1f}")
        personality_table.add_row("Clarity", f"{personality['clarity']:.1f}")
        personality_table.add_row("Expressiveness", f"{personality['expressiveness']:.1f}")
        
        # Musical identity
        musical = state["musical_identity"]
        musical_table = Table(title="Musical Identity")
        musical_table.add_column("Aspect", style="magenta")
        musical_table.add_column("Value", style="bright_white")
        
        musical_table.add_row("Preferred Genres", ", ".join(musical["preferred_genres"]))
        musical_table.add_row("Mood Tendency", musical["mood_tendency"])
        musical_table.add_row("Complexity", f"{musical['complexity']:.1f}")
        musical_table.add_row("Experimental", f"{musical['experimental']:.1f}")
        
        # Display all tables in columns
        self.console.print(Columns([personality_table, musical_table], equal=True))
    
    def display_audio_memories(self):
        """Display recent audio memories"""
        if not self.audio:
            self.console.print("[red]Audio system not initialized[/red]")
            return
        
        self.console.print(Rule("[bold cyan]üíæ Audio Memory Explorer"))
        
        memories = self.audio.get_recent_audio_memories(20)
        
        if not memories:
            self.console.print("[dim]No audio memories recorded yet.[/dim]")
            return
        
        memories_table = Table(title=f"Recent Audio Memories ({len(memories)} total)")
        memories_table.add_column("Time", style="dim")
        memories_table.add_column("Type", style="cyan")
        memories_table.add_column("Content", style="white")
        memories_table.add_column("Emotional Context", style="yellow")
        
        for memory in memories[-10:]:  # Show last 10
            timestamp = memory["timestamp"][:19].replace("T", " ")
            memory_type = memory["type"].replace("_", " ").title()
            
            if memory["type"] == "vocal_expression":
                content = memory["text"][:50] + "..." if len(memory["text"]) > 50 else memory["text"]
                emotion = f"Val:{memory['voice_state']['emotional_valence']:.1f}"
            elif memory["type"] == "musical_creation":
                content = memory["concept"]
                emotion = f"Val:{memory['voice_state']['emotional_valence']:.1f}"
            else:
                content = str(memory).get("content", "Unknown")
                emotion = "N/A"
            
            memories_table.add_row(timestamp, memory_type, content, emotion)
        
        self.console.print(memories_table)
    
    async def run_demo(self):
        """Main demo loop"""
        self.display_banner()
        
        # System status
        self.display_system_status()
        
        # Initialize audio system
        if not await self.initialize_audio():
            return
        
        # Main menu loop
        while True:
            self.console.print("")
            self.display_main_menu()
            
            choice = Prompt.ask(
                "Select demo option",
                choices=[str(i) for i in range(1, 10)],
                default="1"
            )
            
            self.console.print("")
            
            try:
                if choice == "1":
                    await self.test_voice_models()
                elif choice == "2":
                    await self.multi_speaker_dialogue_demo()
                elif choice == "3":
                    await self.musical_consciousness_demo()
                elif choice == "4":
                    self.display_audio_consciousness_state()
                elif choice == "5":
                    self.console.print("[dim]Performance testing not yet implemented[/dim]")
                elif choice == "6":
                    self.console.print("[dim]Voice personality adjustment not yet implemented[/dim]")
                elif choice == "7":
                    self.display_audio_memories()
                elif choice == "8":
                    self.console.print("[dim]Real-time streaming simulation not yet implemented[/dim]")
                elif choice == "9":
                    self.console.print("\n[bright_blue]üéµ Thank you for exploring COCOA's audio consciousness! üéµ[/bright_blue]")
                    break
                    
            except KeyboardInterrupt:
                self.console.print("\n[yellow]Demo interrupted by user[/yellow]")
                break
            except Exception as e:
                self.console.print(f"\n[red]‚ùå Demo error: {e}[/red]")
            
            # Pause before returning to menu
            if choice != "9":
                input("\nPress Enter to continue...")


async def main():
    """Main demo entry point"""
    if not AUDIO_AVAILABLE:
        print("‚ùå Audio system not available")
        return
    
    demo = AudioDemo()
    await demo.run_demo()


if __name__ == "__main__":
    asyncio.run(main())